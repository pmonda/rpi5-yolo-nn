{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45921f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pranesh\\Purdue\\Spring 2025\\ML Research\\rpi5-nn\\rpi5-yolo-nn\\src\\utils.py:340: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes()))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m img = Image.open(\u001b[33m'\u001b[39m\u001b[33mpmh2mmc4.png\u001b[39m\u001b[33m'\u001b[39m).convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     19\u001b[39m sized = img.resize((\u001b[32m416\u001b[39m, \u001b[32m416\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m boxes = \u001b[43mdo_detect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m class_names = load_class_names(\u001b[33m'\u001b[39m\u001b[33mvoc.names\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     24\u001b[39m plot_boxes(img, boxes, \u001b[33m'\u001b[39m\u001b[33mpredict1.jpg\u001b[39m\u001b[33m'\u001b[39m, class_names)  \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Pranesh\\Purdue\\Spring 2025\\ML Research\\rpi5-nn\\rpi5-yolo-nn\\src\\utils.py:364\u001b[39m, in \u001b[36mdo_detect\u001b[39m\u001b[34m(model, img, conf_thresh, nms_thresh, use_cuda)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;66;03m#for j in range(100):\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[38;5;66;03m#    sys.stdout.write('%f ' % (output.storage()[j]))\u001b[39;00m\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m#print('')\u001b[39;00m\n\u001b[32m    362\u001b[39m t3 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m boxes = \u001b[43mget_region_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_anchors\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m#for j in range(len(boxes)):\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;66;03m#    print(boxes[j])\u001b[39;00m\n\u001b[32m    367\u001b[39m t4 = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Pranesh\\Purdue\\Spring 2025\\ML Research\\rpi5-nn\\rpi5-yolo-nn\\src\\utils.py:127\u001b[39m, in \u001b[36mget_region_boxes\u001b[39m\u001b[34m(output, conf_thresh, num_classes, anchors, num_anchors, only_objectness, validation)\u001b[39m\n\u001b[32m    124\u001b[39m all_boxes = []\n\u001b[32m    125\u001b[39m output = output.view(\u001b[38;5;28mint\u001b[39m(batch*num_anchors), \u001b[38;5;28mint\u001b[39m(\u001b[32m5\u001b[39m+num_classes), \u001b[38;5;28mint\u001b[39m(h*w)).transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).contiguous().view(\u001b[38;5;28mint\u001b[39m(\u001b[32m5\u001b[39m+num_classes), \u001b[38;5;28mint\u001b[39m(batch*num_anchors*h*w))\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m grid_x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnum_anchors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnum_anchors\u001b[49m\u001b[43m*\u001b[49m\u001b[43mh\u001b[49m\u001b[43m*\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m grid_y = torch.linspace(\u001b[32m0\u001b[39m, h-\u001b[32m1\u001b[39m, h).repeat(w,\u001b[32m1\u001b[39m).t().repeat(\u001b[38;5;28mint\u001b[39m(batch*num_anchors), \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m).view(\u001b[38;5;28mint\u001b[39m(batch*num_anchors*h*w)).cuda()\n\u001b[32m    129\u001b[39m xs = torch.sigmoid(output[\u001b[32m0\u001b[39m]) + grid_x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Pranesh\\Purdue\\Spring 2025\\ML Research\\rpi5-nn\\rpi5-yolo-nn\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:310\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m     )\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    313\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    314\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from tiny_yolo import *\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from PIL import Image\n",
    "    from utils import *\n",
    "\n",
    "    model = TinyYoloNet()\n",
    "    model.float()\n",
    "    model.load_weights(\"C:/Pranesh/Purdue/Spring 2025/ML Research/rpi5-nn/rpi5-yolo-nn/model_weights/yolov2-tiny-voc.weights\")\n",
    "    model.eval()\n",
    "    # print(m)\n",
    "    \n",
    "    # use_cuda = 1\n",
    "    # if use_cuda:\n",
    "    #     model.cuda()\n",
    "\n",
    "    img = Image.open('pmh2mmc4.png').convert('RGB')\n",
    "    sized = img.resize((416, 416))\n",
    "\n",
    "    boxes = do_detect(model, sized, 0.5, 0.5, use_cuda=False)\n",
    "\n",
    "    class_names = load_class_names('voc.names')\n",
    "    plot_boxes(img, boxes, 'predict1.jpg', class_names)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
